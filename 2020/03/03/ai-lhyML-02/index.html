<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="豆豆">


    <meta name="subtitle" content="愿望是实现睡觉自由">


    <meta name="description" content="你来啦，这里是豆豆的小笔记！">


    <meta name="keywords" content="豆豆,学习,小笔记,睡觉自由">


<title>「AI_02」梯度下降 | 豆豆的小笔记</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">豆豆的小笔记</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">归档</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">豆豆的小笔记</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">归档</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开所有</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">到达底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "收起所有"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "展开所有"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">「AI_02」梯度下降</h1>
            
                <div class="post-meta">
                    

                    
                        <span class="post-time">
                        时间： <a href="#">三月 3, 2020&nbsp;&nbsp;0:40:18</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                        </span>
                    
                    
                        <span class="post-category">
                    分类：
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="复习：梯度下降法"><a href="#复习：梯度下降法" class="headerlink" title="复习：梯度下降法"></a>复习：梯度下降法</h1><p>在之前的第三步中，要解一个优化问题：<br>$$<br>\theta^* = \arg \min_\theta L(\theta)<br>$$</p>
<ul>
<li>$L$：损失函数.</li>
<li>$\theta$：参数.</li>
</ul>
<p>假设$\theta$包含两个变量${\theta_1, \theta_2}$.</p>
<ol>
<li>随机地选择一个起始的$\theta$</li>
</ol>
<p>$$<br>\theta^0 = \left[<br>\begin{matrix}<br>\theta_1^0 \\<br>\theta_2^0<br>\end{matrix}<br>\right]<br>$$</p>
<ol start="2">
<li>计算下一个点的坐标</li>
</ol>
<p>$$<br>\left[<br>\begin{matrix}<br>\theta_1^1 \\<br>\theta_2^1<br>\end{matrix}<br>\right] = \left[<br>\begin{matrix}<br>\theta_1^0 \\<br>\theta_2^0<br>\end{matrix}<br>\right] - \eta \left[<br>\begin{matrix}<br>\frac{\partial L(\theta_1^0)}{\partial \theta_1} \\<br>\frac{\partial L(\theta_2^0)}{\partial \theta_2}<br>\end{matrix}<br>\right]<br>$$</p>
<ol start="3">
<li>反复进行.</li>
</ol>
<blockquote>
<p>梯度表示为<br>$$<br>\nabla L(\theta) = \left[<br>\begin{matrix}<br>\frac{\partial C(\theta_1)}{\partial \theta_1} \\<br>\frac{\partial C(\theta_2)}{\partial \theta_2}<br>\end{matrix}<br>\right]<br>$$</p>
</blockquote>
<p>那么点的更新过程就可以写成：</p>
<p>$$<br>\theta^1 = \theta^0 - \eta \nabla L(\theta^0)<br>$$</p>
<p>$$<br>\theta^2 = \theta^1 - \eta \nabla L(\theta^1)<br>$$</p>
<h1 id="提示一：小心调整步长"><a href="#提示一：小心调整步长" class="headerlink" title="提示一：小心调整步长"></a>提示一：小心调整步长</h1><p>如果步长过短，可能会需要很长时间才能收敛；如果步长过长，可能无法收敛到最优点.</p>
<p>对于多维的目标函数，可能无法可视化迭代的过程，但是可以可视化<strong>不同的步长下Loss值和迭代次数的关系图</strong>.</p>
<h2 id="自适应调整步长"><a href="#自适应调整步长" class="headerlink" title="自适应调整步长"></a>自适应调整步长</h2><ul>
<li>一种常用的做法：<strong>每隔几次迭代就减小步长</strong>.<ul>
<li>最开始的时候，我们距离最优点很远，所以我们可以使用很大的步长.</li>
<li>经过若干次迭代后，我们距离最优点很近了，所以我们要减小步长.</li>
<li>例如：$\frac{1}{t}$衰减，$\eta^t = \frac{\eta}{\sqrt{t+1}}$，其中$t$代表迭代次数.</li>
</ul>
</li>
<li>步长的选择是不可能一劳永逸的.<ul>
<li>最好的做法是，<strong>每个参数选择不同的步长</strong>. </li>
</ul>
</li>
</ul>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>将每一个参数的步长都除以它之前导数的均方根.</p>
<p>批次梯度下降（Vanilla Gradient descent）的做法：<br>$$<br>w^t - \eta^t g^t \rightarrow w^{t+1}<br>$$<br>改进版算法（Adagrad）的做法：<br>$$<br>w^t - \frac{\eta^t}{\sigma^t} g^t \rightarrow w^{t+1}<br>$$</p>
<ul>
<li>$w$：是其中一个参数，因为改进的算法要针对每个参数选择不同的步长.</li>
<li>$\sigma^t$：参数$w$的前一个导数的均方根.</li>
<li>$\eta^t = \frac{\eta}{\sqrt{t+1}}$.</li>
<li>$g^t = \frac{\partial C(\theta^t)}{\partial w}$.</li>
</ul>
<p>这样以来，每次参数的更新都是依赖于每个参数的不同梯度.</p>
<p>例子：<br>$$<br>w^1 \leftarrow w^0 - \frac{\eta^0}{\sigma^0} g^0 , \sigma^0 = \sqrt{(g^0)^2}<br>$$</p>
<p>$$<br>w^2 \leftarrow w^1 - \frac{\eta^1}{\sigma^1} g^1 , \sigma^1 = \sqrt{\frac{1}{2}[(g^0)^2+(g^1)^2]}<br>$$</p>
<p>$$<br>w^3 \leftarrow w^2 - \frac{\eta^2}{\sigma^2} g^2 , \sigma^2 = \sqrt{\frac{1}{3}[(g^0)^2+(g^1)^2+(g^2)^2]}<br>$$</p>
<p>$$<br>\dots \dots<br>$$</p>
<p>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta^t}{\sigma^t} g^t , \sigma^t = \sqrt{\frac{1}{t+1} \sum_{i=0}^{t}(g^i)^2}<br>$$</p>
<p>将$\eta^t$和$\sigma^t$的表达式代入到式中，上下同时消去$\frac{1}{\sqrt{t+1}}$得：<br>$$<br>w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}} g^t<br>$$</p>
<h2 id="矛盾"><a href="#矛盾" class="headerlink" title="矛盾"></a>矛盾</h2><p>在上面得到的式子里，梯度$g^t$越大，他每次迭代的变化越大，但是分母$\sqrt{\sum_{i=0}^{t}(g^i)^2}$就会越小.</p>
<h3 id="直观的解释"><a href="#直观的解释" class="headerlink" title="直观的解释"></a>直观的解释</h3><p>例如：</p>
<table>
<thead>
<tr>
<th align="center">$g^0$</th>
<th align="center">$g^1$</th>
<th align="center">$g^2$</th>
<th align="center">$g^3$</th>
<th align="center">$g^4$</th>
<th align="center">$\dots$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.001</td>
<td align="center">0.001</td>
<td align="center">0.003</td>
<td align="center">0.002</td>
<td align="center">0.1</td>
<td align="center">$\dots$</td>
</tr>
</tbody></table>
<p>$g^4$的反差特别大.</p>
<table>
<thead>
<tr>
<th align="center">$g^0$</th>
<th align="center">$g^1$</th>
<th align="center">$g^2$</th>
<th align="center">$g^3$</th>
<th align="center">$g^4$</th>
<th align="center">$\dots$</th>
</tr>
</thead>
<tbody><tr>
<td align="center">10.8</td>
<td align="center">20.9</td>
<td align="center">31.7</td>
<td align="center">12.1</td>
<td align="center">0.1</td>
<td align="center">$\dots$</td>
</tr>
</tbody></table>
<p>$g^4$的反差特别小.</p>
<p>分母$\sqrt{\sum_{i=0}^{t}(g^i)^2}$就表示了造成反差的效果.</p>
<h1 id="提示二：随机梯度下降法"><a href="#提示二：随机梯度下降法" class="headerlink" title="提示二：随机梯度下降法"></a>提示二：随机梯度下降法</h1><p>原来的梯度下降方法：</p>
<ul>
<li>损失函数的计算：</li>
</ul>
<p>$$<br>L = \sum_{n} \left(\hat{y}^n - \left(b + \sum w_i x_i^n\right)\right)^2<br>$$</p>
<ul>
<li>点的更新：</li>
</ul>
<p>$$<br>\theta^i = \theta^{i-1} - \eta \nabla L(\theta^{i-1})<br>$$</p>
<p>而随机梯度下降法（Stochastic Gradient Descent）：</p>
<ul>
<li>选择一个样本$x^n$.</li>
<li>只计算选择的样本$x^n$的损失函数：</li>
</ul>
<p>$$<br>L^n = \left( \hat{y}^n - \left( b + \sum w_i x_i^n \right) \right)^2<br>$$</p>
<ul>
<li>根据样本$x^n$的损失函数来更新点：</li>
</ul>
<p>$$<br>\theta^i = \theta^{i-1} - \eta \nabla L^n(\theta^{i-1})<br>$$</p>
<p>这样，在原来的梯度下降法看了全部样本之前，就可以看一个样本走一步，最后得到解.</p>
<h1 id="提示三：特征放缩"><a href="#提示三：特征放缩" class="headerlink" title="提示三：特征放缩"></a>提示三：特征放缩</h1><p>例如，函数<br>$$<br>y = b + w_1 x_1 + w_2 x_2<br>$$<br>当中，$x_1$和$x_2$的尺度不同，那么就要进行特征放缩，让特征拥有相同的尺度.</p>
<hr>
<p>一种常见的特征放缩的方法：</p>
<p>假设有$R$个<strong>样本</strong>，每个样本中都有一组<strong>特征</strong>：</p>
<ul>
<li>$x^1={x_1^1, x_2^1, x_3^1, \dots , x_n^1}$.</li>
<li>$x^2={x_1^2, x_2^2, x_3^2, \dots , x_n^2}$.</li>
<li>$x^3={x_1^3, x_2^3, x_3^3, \dots , x_n^3}$.</li>
<li>$\dots \dots$</li>
<li>$x^r={x_1^r, x_2^r, x_3^r, \dots , x_n^r}$.</li>
<li>$\dots \dots $</li>
<li>$x^R={x_1^R, x_2^R, x_3^R, \dots , x_n^R}$.</li>
</ul>
<p>对于每一个维度$i$的特征，计算：</p>
<ol>
<li>均值：</li>
</ol>
<p>$$<br>m_i = \frac{1}{R} \sum^{R}_{j=1} x_i^j<br>$$</p>
<ol start="2">
<li>标准差：</li>
</ol>
<p>$$<br>\sigma_i = \frac{1}{R}\sum_{j=1}^{R} \left( x_i^j - m_i \right)^2<br>$$</p>
<ol start="3">
<li>计算尺度变换后的特征：</li>
</ol>
<p>$$<br>x_i^r = \dfrac{x_i^r - m_i}{\sigma_i}<br>$$</p>
<p>这样得到的新的特征<strong>均值为0，方差为1</strong>.</p>

        </div>

        
        <section class="post-tags">
            <div>
                <span>标签：</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9D%8E%E5%AE%8F%E6%AF%85/"># 李宏毅</a>&nbsp;&nbsp;
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>&nbsp;&nbsp;
                    
                        <a href="/tags/AI/"># AI</a>&nbsp;&nbsp;
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span>· </span>
                <a href="/">主页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/03/09/go-01/">「Go_01」搭建Go语言开发环境</a>
            
            
            <a class="next" rel="next" href="/2020/03/02/ai-lhyML-01/">「AI_01」回归：案例学习</a>
            
        </section>


    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 豆豆 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>


    <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
    <script>
      if (window.mermaid) {
        mermaid.initialize({theme: 'neutral'});
      }
    </script>
  
    </div>
</body>
</html>
